{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación de Nuevas Variables - Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/02/22 14:47:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/02/22 14:47:17 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "/home/aldos/anaconda3/lib/python3.9/site-packages/pyspark/sql/context.py:77: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext=SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "bd5 = sqlContext.read.format(\n",
    "    \"com.databricks.spark.csv\"\n",
    ").option(\"header\", \"true\").load(\"../bd5.csv\", inferSchema=True)\n",
    "sqlContext.registerDataFrameAsTable(bd5, \"bd5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables Dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bd5 = bd5.withColumn('Horario1',(bd5.Horario==1) \n",
    ").withColumn('Horario2',(bd5.Horario==2) \n",
    ").withColumn('Horario3',(bd5.Horario==3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables Discretizadas Binarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/22 14:48:37 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(Year=2016, Month=12, DayofMonth=1, DayOfWeek=4, CRSDepTime=845, UniqueCarrier='AA', TailNum='N8ARAA', ArrDelay=-7.0, DepDelay=-5.0, Origin='LAX', Dest='DFW', Distance=1235.0, Cancelled=0.0, Diverted=0.0, CarrierDelay=0.0, WeatherDelay=0.0, NASDelay=0.0, SecurityDelay=0.0, LateAircraftDelay=0.0, LogD=3.0916669575956846, Retraso=0, RetrasoNeto=-2.0, Horario=2, Horario1=False, Horario2=True, Horario3=False, SalidaBin=0.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "\n",
    "binarizer = Binarizer(threshold=15.0, inputCol='DepDelay', outputCol='SalidaBin')\n",
    "binarizer.transform(bd5).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|DepDelay|SalidaBin|\n",
      "+--------+---------+\n",
      "|    -5.0|      0.0|\n",
      "|     5.0|      0.0|\n",
      "|    -3.0|      0.0|\n",
      "|    -7.0|      0.0|\n",
      "|    -6.0|      0.0|\n",
      "|    -1.0|      0.0|\n",
      "|     0.0|      0.0|\n",
      "|     0.0|      0.0|\n",
      "|    -1.0|      0.0|\n",
      "|    -1.0|      0.0|\n",
      "|     1.0|      0.0|\n",
      "|    -2.0|      0.0|\n",
      "|    -4.0|      0.0|\n",
      "|    -1.0|      0.0|\n",
      "|     0.0|      0.0|\n",
      "|     0.0|      0.0|\n",
      "|    13.0|      0.0|\n",
      "|    17.0|      1.0|\n",
      "|    12.0|      0.0|\n",
      "|    19.0|      1.0|\n",
      "+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "binarizer.transform(bd5).select('DepDelay','SalidaBin').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables Discretizadas en Buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|DepDelay|SalidaCat|\n",
      "+--------+---------+\n",
      "|    -5.0|      0.0|\n",
      "|     5.0|      1.0|\n",
      "|    -3.0|      0.0|\n",
      "|    -7.0|      0.0|\n",
      "|    -6.0|      0.0|\n",
      "|    -1.0|      0.0|\n",
      "|     0.0|      1.0|\n",
      "|     0.0|      1.0|\n",
      "|    -1.0|      0.0|\n",
      "|    -1.0|      0.0|\n",
      "|     1.0|      1.0|\n",
      "|    -2.0|      0.0|\n",
      "|    -4.0|      0.0|\n",
      "|    -1.0|      0.0|\n",
      "|     0.0|      1.0|\n",
      "|     0.0|      1.0|\n",
      "|    13.0|      1.0|\n",
      "|    17.0|      2.0|\n",
      "|    12.0|      1.0|\n",
      "|    19.0|      2.0|\n",
      "+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "bucketizer = Bucketizer(splits=[-float(\"inf\"), 0.0, 15.0, float(\"inf\")],\n",
    "                        inputCol='DepDelay', outputCol='SalidaCat')\n",
    "bucketizer.transform(bd5).select('DepDelay','SalidaCat').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Versiones más nuevas de Pyspark incluyen otras transformaciones, por ejemplo QuantileDiscretizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expansión polinómica de Variables \n",
    "(términos cuadráticos, productos, etc.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DepDelay=-5.0, Distance=1235.0, Polyn=DenseVector([-5.0, 25.0, 1235.0, -6175.0, 1525225.0])),\n",
       " Row(DepDelay=5.0, Distance=1235.0, Polyn=DenseVector([5.0, 25.0, 1235.0, 6175.0, 1525225.0])),\n",
       " Row(DepDelay=-3.0, Distance=1235.0, Polyn=DenseVector([-3.0, 9.0, 1235.0, -3705.0, 1525225.0])),\n",
       " Row(DepDelay=-7.0, Distance=1235.0, Polyn=DenseVector([-7.0, 49.0, 1235.0, -8645.0, 1525225.0])),\n",
       " Row(DepDelay=-6.0, Distance=1235.0, Polyn=DenseVector([-6.0, 36.0, 1235.0, -7410.0, 1525225.0]))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import PolynomialExpansion\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['DepDelay','Distance'],\n",
    "    outputCol='features')\n",
    "\n",
    "px = PolynomialExpansion(\n",
    "    degree=2, \n",
    "    inputCol=\"features\", \n",
    "    outputCol=\"Polyn\")\n",
    "\n",
    "bd6 = px.transform(assembler.transform(bd5))\n",
    "\n",
    "bd6.select('DepDelay','Distance','Polyn').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30466"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bd6.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estandarización de las variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|     features|         stdfeatures|\n",
      "+-------------+--------------------+\n",
      "|[-5.0,1235.0]|[-0.4459454808573...|\n",
      "| [5.0,1235.0]|[-0.2452533483159...|\n",
      "|[-3.0,1235.0]|[-0.4058070543490...|\n",
      "|[-7.0,1235.0]|[-0.4860839073656...|\n",
      "|[-6.0,1235.0]|[-0.4660146941114...|\n",
      "|[-1.0,1235.0]|[-0.3656686278407...|\n",
      "| [0.0,1235.0]|[-0.3455994145866...|\n",
      "| [0.0,1235.0]|[-0.3455994145866...|\n",
      "|[-1.0,1235.0]|[-0.3656686278407...|\n",
      "|[-1.0,1235.0]|[-0.3656686278407...|\n",
      "| [1.0,1235.0]|[-0.3255302013325...|\n",
      "|[-2.0,1235.0]|[-0.3857378410949...|\n",
      "|[-4.0,1235.0]|[-0.4258762676032...|\n",
      "|[-1.0,1235.0]|[-0.3656686278407...|\n",
      "| [0.0,1235.0]|[-0.3455994145866...|\n",
      "| [0.0,1235.0]|[-0.3455994145866...|\n",
      "|[13.0,1235.0]|[-0.0846996422828...|\n",
      "|[17.0,1235.0]|[-0.0044227892663...|\n",
      "|[12.0,1235.0]|[-0.1047688555370...|\n",
      "|[19.0,1235.0]|[0.03571563724193...|\n",
      "+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"stdfeatures\",\n",
    "                        withStd=True, withMean=True)\n",
    "scalerModel = scaler.fit(bd6)\n",
    "bd6std = scalerModel.transform(bd6)\n",
    "\n",
    "bd6std.select('features','stdfeatures').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tranformación manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "bd7 = bd6.withColumn('DepDelay2',(bd6.DepDelay**2)\n",
    ").withColumn('DepD_Distance',(bd6.DepDelay * bd6.Distance)) "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
